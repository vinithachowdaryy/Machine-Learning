{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Feature Index: 254\n","Best Information Gain: 0.14431896891734253\n"]}],"source":["import numpy as np\n","from collections import Counter\n","from math import log2\n","\n","def load_data(features_file, labels_file):\n","    \"\"\"\n","    Load features and labels from the provided files.\n","    \"\"\"\n","    extracted_features = np.load(features_file)\n","    labels = np.load(labels_file)\n","    return extracted_features, labels\n","\n","def convert_to_categorical(features, num_bins=5):\n","    \"\"\"\n","    Convert numerical features to categorical by binning.\n","    \"\"\"\n","    categorical_features = []\n","    for feature in features.T:  # Transpose to loop through columns\n","        bins = np.linspace(min(feature), max(feature), num_bins + 1)\n","        digitized = np.digitize(feature, bins)\n","        categorical_features.append(digitized)\n","    return np.array(categorical_features).T\n","\n","def calculate_entropy(labels):\n","    \"\"\"\n","    Calculate the entropy of a set of labels.\n","    \"\"\"\n","    label_counts = Counter(labels)\n","    total_samples = len(labels)\n","    \n","    entropy = 0\n","    for label in label_counts:\n","        prob = label_counts[label] / total_samples\n","        entropy -= prob * log2(prob)\n","    \n","    return entropy\n","\n","def calculate_information_gain(feature_column, labels):\n","    \"\"\"\n","    Calculate Information Gain for a feature.\n","    \"\"\"\n","    # Calculate total entropy of the dataset\n","    total_entropy = calculate_entropy(labels)\n","    \n","    # Calculate the weighted entropy of each unique value in the feature\n","    unique_values = np.unique(feature_column)\n","    weighted_entropy = 0\n","    for value in unique_values:\n","        subset_indices = np.where(feature_column == value)[0]\n","        subset_labels = labels[subset_indices]\n","        weight = len(subset_labels) / len(labels)\n","        subset_entropy = calculate_entropy(subset_labels)\n","        weighted_entropy += weight * subset_entropy\n","    \n","    # Calculate Information Gain\n","    information_gain = total_entropy - weighted_entropy\n","    return information_gain\n","\n","def find_best_feature(extracted_features, labels):\n","    \"\"\"\n","    Find the best feature for the root node based on Information Gain.\n","    \"\"\"\n","    num_features = extracted_features.shape[1]\n","    best_feature_index = -1\n","    best_information_gain = -1\n","    \n","    for i in range(num_features):\n","        feature_column = extracted_features[:, i]\n","        information_gain = calculate_information_gain(feature_column, labels)\n","        \n","        if information_gain > best_information_gain:\n","            best_information_gain = information_gain\n","            best_feature_index = i\n","    \n","    return best_feature_index, best_information_gain\n","\n","def main():\n","    # Load data\n","    features_file = \"D:\\SEM-4\\ML\\CODES\\Machine-Learning\\Lab04\\extracted_features.npy\"\n","    labels_file = \"D:\\SEM-4\\ML\\CODES\\Machine-Learning\\Lab04\\labels.npy\"\n","    extracted_features, labels = load_data(features_file, labels_file)\n","    \n","    # Convert features to categorical if needed\n","    categorical_features = convert_to_categorical(extracted_features)\n","    \n","    # Find the best feature for the root node\n","    best_feature_index, best_information_gain = find_best_feature(categorical_features, labels)\n","    \n","    print(\"Best Feature Index:\", best_feature_index)\n","    print(\"Best Information Gain:\", best_information_gain)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Feature Index (Equal Width Binning): 254\n","Best Information Gain (Equal Width Binning): 0.14431896891734253\n","\n","Best Feature Index (Equal Frequency Binning): 254\n","Best Information Gain (Equal Frequency Binning): 0.1590929509346024\n"]}],"source":["import numpy as np\n","from collections import Counter\n","from math import log2\n","\n","def load_data(features_file, labels_file):\n","    \"\"\"\n","    Load features and labels from the provided files.\n","    \"\"\"\n","    extracted_features = np.load(features_file)\n","    labels = np.load(labels_file)\n","    return extracted_features, labels\n","\n","def convert_to_categorical(features, binning_type=None, num_bins=None):\n","   \n","    if binning_type is None:\n","        binning_type = 'equal_width'\n","    if num_bins is None:\n","        num_bins = 5\n","    \n","    categorical_features = []\n","    for feature in features.T:  # Transpose to loop through columns\n","        if binning_type == 'equal_width':\n","            bins = np.linspace(min(feature), max(feature), num_bins + 1)\n","        elif binning_type == 'equal_frequency':\n","            bins = np.quantile(feature, np.linspace(0, 1, num_bins + 1))\n","        else:\n","            raise ValueError(\"Invalid binning type. Use 'equal_width' or 'equal_frequency'.\")\n","        \n","        digitized = np.digitize(feature, bins)\n","        categorical_features.append(digitized)\n","    \n","    return np.array(categorical_features).T\n","\n","def calculate_entropy(labels):\n","    \"\"\"\n","    Calculate the entropy of a set of labels.\n","    \"\"\"\n","    label_counts = Counter(labels)\n","    total_samples = len(labels)\n","    \n","    entropy = 0\n","    for label in label_counts:\n","        prob = label_counts[label] / total_samples\n","        entropy -= prob * log2(prob)\n","    \n","    return entropy\n","\n","def calculate_information_gain(feature_column, labels):\n","    \"\"\"\n","    Calculate Information Gain for a feature.\n","    \"\"\"\n","    # Calculate total entropy of the dataset\n","    total_entropy = calculate_entropy(labels)\n","    \n","    # Calculate the weighted entropy of each unique value in the feature\n","    unique_values = np.unique(feature_column)\n","    weighted_entropy = 0\n","    for value in unique_values:\n","        subset_indices = np.where(feature_column == value)[0]\n","        subset_labels = labels[subset_indices]\n","        weight = len(subset_labels) / len(labels)\n","        subset_entropy = calculate_entropy(subset_labels)\n","        weighted_entropy += weight * subset_entropy\n","    \n","    # Calculate Information Gain\n","    information_gain = total_entropy - weighted_entropy\n","    return information_gain\n","\n","def find_best_feature(extracted_features, labels):\n","    \"\"\"\n","    Find the best feature for the root node based on Information Gain.\n","    \"\"\"\n","    num_features = extracted_features.shape[1]\n","    best_feature_index = -1\n","    best_information_gain = -1\n","    \n","    for i in range(num_features):\n","        feature_column = extracted_features[:, i]\n","        information_gain = calculate_information_gain(feature_column, labels)\n","        \n","        if information_gain > best_information_gain:\n","            best_information_gain = information_gain\n","            best_feature_index = i\n","    \n","    return best_feature_index, best_information_gain\n","\n","def main():\n","    # Load data\n","    features_file = \"D:\\SEM-4\\ML\\CODES\\Machine-Learning\\Lab04\\extracted_features.npy\"\n","    labels_file = \"D:\\SEM-4\\ML\\CODES\\Machine-Learning\\Lab04\\labels.npy\"\n","    extracted_features, labels = load_data(features_file, labels_file)\n","    \n","    # Convert features to categorical with default parameters\n","    categorical_features_equal_width = convert_to_categorical(extracted_features)\n","    \n","    # Find the best feature for the root node\n","    best_feature_index, best_information_gain = find_best_feature(categorical_features_equal_width, labels)\n","    \n","    print(\"Best Feature Index (Equal Width Binning):\", best_feature_index)\n","    print(\"Best Information Gain (Equal Width Binning):\", best_information_gain)\n","    \n","    # Convert features to categorical with equal frequency binning, 7 bins\n","    categorical_features_equal_freq = convert_to_categorical(extracted_features, binning_type='equal_frequency', num_bins=7)\n","    \n","    # Find the best feature for the root node\n","    best_feature_index_freq, best_information_gain_freq = find_best_feature(categorical_features_equal_freq, labels)\n","    \n","    print(\"\\nBest Feature Index (Equal Frequency Binning):\", best_feature_index_freq)\n","    print(\"Best Information Gain (Equal Frequency Binning):\", best_information_gain_freq)\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Decision Tree:\n"," Feature 254\n","   Value 1 ->      Feature 315\n","       Value 1 ->          Feature 494\n","           Value 1 ->              Predicted Label: 6\n","           Value 2 ->              Predicted Label: 1\n","           Value 3 ->              Predicted Label: 3\n","           Value 4 ->              Predicted Label: 0\n","           Value 5 ->              Predicted Label: 9\n","       Value 2 ->          Feature 15\n","           Value 1 ->              Predicted Label: 8\n","           Value 2 ->              Predicted Label: 1\n","           Value 3 ->              Predicted Label: 1\n","           Value 4 ->              Predicted Label: 1\n","           Value 5 ->              Predicted Label: 4\n","           Value 6 ->              Predicted Label: 1\n","       Value 3 ->          Feature 276\n","           Value 1 ->              Predicted Label: 8\n","           Value 2 ->              Predicted Label: 5\n","           Value 3 ->              Predicted Label: 1\n","           Value 4 ->              Predicted Label: 1\n","           Value 5 ->              Predicted Label: 1\n","       Value 4 ->          Feature 55\n","           Value 1 ->              Predicted Label: 5\n","           Value 2 ->              Predicted Label: 5\n","           Value 3 ->              Predicted Label: 5\n","           Value 4 ->              Predicted Label: 10\n","           Value 5 ->              Predicted Label: 8\n","       Value 5 ->          Feature 482\n","           Value 1 ->              Predicted Label: 5\n","           Value 2 ->              Predicted Label: 1\n","           Value 3 ->              Predicted Label: 12\n","           Value 4 ->              Predicted Label: 12\n","   Value 2 ->      Feature 155\n","       Value 1 ->          Feature 15\n","           Value 1 ->              Predicted Label: 8\n","           Value 2 ->              Predicted Label: 5\n","           Value 3 ->              Predicted Label: 5\n","           Value 4 ->              Predicted Label: 8\n","       Value 2 ->          Feature 172\n","           Value 1 ->              Predicted Label: 10\n","           Value 2 ->              Predicted Label: 8\n","           Value 3 ->              Predicted Label: 8\n","           Value 4 ->              Predicted Label: 5\n","           Value 5 ->              Predicted Label: 5\n","       Value 3 ->          Feature 172\n","           Value 1 ->              Predicted Label: 10\n","           Value 2 ->              Predicted Label: 8\n","           Value 3 ->              Predicted Label: 4\n","           Value 4 ->              Predicted Label: 1\n","           Value 5 ->              Predicted Label: 1\n","       Value 4 ->          Feature 15\n","           Value 1 ->              Predicted Label: 8\n","           Value 2 ->              Predicted Label: 4\n","           Value 3 ->              Predicted Label: 4\n","           Value 4 ->              Predicted Label: 4\n","           Value 5 ->              Predicted Label: 1\n","       Value 5 ->          Feature 109\n","           Value 1 ->              Predicted Label: 4\n","           Value 2 ->              Predicted Label: 8\n","           Value 3 ->              Predicted Label: 8\n","           Value 4 ->              Predicted Label: 8\n","           Value 5 ->              Predicted Label: 10\n","       Value 6 ->          Predicted Label: 0\n","   Value 3 ->      Feature 315\n","       Value 1 ->          Feature 494\n","           Value 1 ->              Predicted Label: 3\n","           Value 2 ->              Predicted Label: 8\n","           Value 3 ->              Predicted Label: 8\n","           Value 4 ->              Predicted Label: 8\n","           Value 5 ->              Predicted Label: 9\n","       Value 2 ->          Feature 403\n","           Value 1 ->              Predicted Label: 8\n","           Value 2 ->              Predicted Label: 2\n","           Value 3 ->              Predicted Label: 8\n","           Value 4 ->              Predicted Label: 2\n","           Value 5 ->              Predicted Label: 7\n","       Value 3 ->          Feature 56\n","           Value 1 ->              Predicted Label: 8\n","           Value 2 ->              Predicted Label: 2\n","           Value 3 ->              Predicted Label: 9\n","           Value 4 ->              Predicted Label: 9\n","           Value 5 ->              Predicted Label: 7\n","       Value 4 ->          Feature 56\n","           Value 1 ->              Predicted Label: 7\n","           Value 2 ->              Predicted Label: 10\n","           Value 3 ->              Predicted Label: 2\n","           Value 4 ->              Predicted Label: 9\n","           Value 5 ->              Predicted Label: 9\n","       Value 5 ->          Feature 56\n","           Value 1 ->              Predicted Label: 7\n","           Value 2 ->              Predicted Label: 10\n","           Value 3 ->              Predicted Label: 10\n","           Value 4 ->              Predicted Label: 10\n","           Value 5 ->              Predicted Label: 10\n","           Value 6 ->              Predicted Label: 7\n","       Value 6 ->          Predicted Label: 10\n","   Value 4 ->      Feature 315\n","       Value 1 ->          Predicted Label: 3\n","       Value 2 ->          Feature 2\n","           Value 1 ->              Predicted Label: 8\n","           Value 2 ->              Predicted Label: 2\n","           Value 3 ->              Predicted Label: 2\n","           Value 4 ->              Predicted Label: 8\n","           Value 5 ->              Predicted Label: 8\n","           Value 6 ->              Predicted Label: 8\n","       Value 3 ->          Feature 103\n","           Value 1 ->              Predicted Label: 2\n","           Value 2 ->              Predicted Label: 2\n","           Value 3 ->              Predicted Label: 2\n","           Value 4 ->              Predicted Label: 2\n","           Value 5 ->              Predicted Label: 2\n","       Value 4 ->          Feature 377\n","           Value 1 ->              Predicted Label: 10\n","           Value 2 ->              Predicted Label: 7\n","           Value 3 ->              Predicted Label: 7\n","           Value 4 ->              Predicted Label: 3\n","       Value 5 ->          Feature 56\n","           Value 1 ->              Predicted Label: 7\n","           Value 2 ->              Predicted Label: 10\n","           Value 3 ->              Predicted Label: 10\n","           Value 4 ->              Predicted Label: 10\n","           Value 5 ->              Predicted Label: 10\n","   Value 5 ->      Feature 315\n","       Value 2 ->          Feature 36\n","           Value 1 ->              Predicted Label: 8\n","           Value 2 ->              Predicted Label: 0\n","           Value 3 ->              Predicted Label: 9\n","           Value 4 ->              Predicted Label: 2\n","           Value 5 ->              Predicted Label: 8\n","       Value 3 ->          Feature 441\n","           Value 2 ->              Predicted Label: 11\n","           Value 3 ->              Predicted Label: 2\n","           Value 4 ->              Predicted Label: 2\n","           Value 5 ->              Predicted Label: 3\n","       Value 4 ->          Feature 139\n","           Value 1 ->              Predicted Label: 7\n","           Value 2 ->              Predicted Label: 10\n","           Value 3 ->              Predicted Label: 10\n","           Value 4 ->              Predicted Label: 10\n","           Value 5 ->              Predicted Label: 10\n","           Value 6 ->              Predicted Label: 2\n","       Value 5 ->          Feature 229\n","           Value 1 ->              Predicted Label: 10\n","           Value 2 ->              Predicted Label: 7\n","   Value 6 ->      Predicted Label: 11\n"]}],"source":["import numpy as np\n","from collections import Counter\n","from math import log2\n","\n","class Node:\n","    \"\"\"\n","    Node class for the Decision Tree.\n","    \"\"\"\n","    def __init__(self, feature_index=None, value=None, is_leaf=False, label=None):\n","        self.feature_index = feature_index  # Index of the feature\n","        self.value = value  # Value of the split\n","        self.is_leaf = is_leaf  # True if it's a leaf node\n","        self.label = label  # Predicted label if it's a leaf\n","        self.children = {}  # Dictionary to store children nodes\n","\n","def load_data(features_file, labels_file):\n","    \"\"\"\n","    Load features and labels from the provided files.\n","    \"\"\"\n","    extracted_features = np.load(features_file)\n","    labels = np.load(labels_file)\n","    return extracted_features, labels\n","\n","def convert_to_categorical(features, binning_type=None, num_bins=None):\n","\n","    if binning_type is None:\n","        binning_type = 'equal_width'\n","    if num_bins is None:\n","        num_bins = 5\n","    \n","    categorical_features = []\n","    for feature in features.T:  # Transpose to loop through columns\n","        if binning_type == 'equal_width':\n","            bins = np.linspace(min(feature), max(feature), num_bins + 1)\n","        elif binning_type == 'equal_frequency':\n","            bins = np.quantile(feature, np.linspace(0, 1, num_bins + 1))\n","        else:\n","            raise ValueError(\"Invalid binning type. Use 'equal_width' or 'equal_frequency'.\")\n","        \n","        digitized = np.digitize(feature, bins)\n","        categorical_features.append(digitized)\n","    \n","    return np.array(categorical_features).T\n","\n","def calculate_entropy(labels):\n","    \"\"\"\n","    Calculate the entropy of a set of labels.\n","    \"\"\"\n","    label_counts = Counter(labels)\n","    total_samples = len(labels)\n","    \n","    entropy = 0\n","    for label in label_counts:\n","        prob = label_counts[label] / total_samples\n","        entropy -= prob * log2(prob)\n","    \n","    return entropy\n","\n","def calculate_information_gain(feature_column, labels):\n","    \"\"\"\n","    Calculate Information Gain for a feature.\n","    \"\"\"\n","    # Calculate total entropy of the dataset\n","    total_entropy = calculate_entropy(labels)\n","    \n","    # Calculate the weighted entropy of each unique value in the feature\n","    unique_values = np.unique(feature_column)\n","    weighted_entropy = 0\n","    for value in unique_values:\n","        subset_indices = np.where(feature_column == value)[0]\n","        subset_labels = labels[subset_indices]\n","        weight = len(subset_labels) / len(labels)\n","        subset_entropy = calculate_entropy(subset_labels)\n","        weighted_entropy += weight * subset_entropy\n","    \n","    # Calculate Information Gain\n","    information_gain = total_entropy - weighted_entropy\n","    return information_gain\n","\n","def find_best_feature(extracted_features, labels):\n","    \"\"\"\n","    Find the best feature for the root node based on Information Gain.\n","    \"\"\"\n","    num_features = extracted_features.shape[1]\n","    best_feature_index = -1\n","    best_information_gain = -1\n","    \n","    for i in range(num_features):\n","        feature_column = extracted_features[:, i]\n","        information_gain = calculate_information_gain(feature_column, labels)\n","        \n","        if information_gain > best_information_gain:\n","            best_information_gain = information_gain\n","            best_feature_index = i\n","    \n","    return best_feature_index, best_information_gain\n","\n","def build_tree(extracted_features, labels, max_depth=None, min_samples_split=2):\n","   \n","    if max_depth is None:\n","        max_depth = float('inf')\n","    \n","    if len(np.unique(labels)) == 1 or len(labels) < min_samples_split or max_depth == 0:\n","        # Create a leaf node\n","        return Node(is_leaf=True, label=Counter(labels).most_common(1)[0][0])\n","    \n","    best_feature_index, best_information_gain = find_best_feature(extracted_features, labels)\n","    \n","    if best_information_gain == 0:\n","        # Create a leaf node\n","        return Node(is_leaf=True, label=Counter(labels).most_common(1)[0][0])\n","    \n","    best_feature_values = np.unique(extracted_features[:, best_feature_index])\n","    \n","    root = Node(feature_index=best_feature_index)\n","    \n","    for value in best_feature_values:\n","        subset_indices = np.where(extracted_features[:, best_feature_index] == value)[0]\n","        subset_features = extracted_features[subset_indices]\n","        subset_labels = labels[subset_indices]\n","        \n","        child = build_tree(subset_features, subset_labels, max_depth - 1, min_samples_split)\n","        root.children[value] = child\n","    \n","    return root\n","\n","def print_tree(node, depth=0):\n","    \"\"\"\n","    Print the Decision Tree.\n","    \"\"\"\n","    if node.is_leaf:\n","        print(\"  \" * depth, \"Predicted Label:\", node.label)\n","    else:\n","        print(\"  \" * depth, \"Feature\", node.feature_index)\n","        for value, child_node in node.children.items():\n","            print(\"  \" * (depth + 1), \"Value\", value, \"->\", end=\" \")\n","            print_tree(child_node, depth + 2)\n","\n","def main():\n","    # Load data\n","    features_file = \"D:\\SEM-4\\ML\\CODES\\Machine-Learning\\Lab04\\extracted_features.npy\"\n","    labels_file = \"D:\\SEM-4\\ML\\CODES\\Machine-Learning\\Lab04\\labels.npy\"\n","    extracted_features, labels = load_data(features_file, labels_file)\n","    \n","    # Convert features to categorical with default parameters\n","    categorical_features = convert_to_categorical(extracted_features)\n","    \n","    # Build the Decision Tree\n","    decision_tree = build_tree(categorical_features, labels, max_depth=3, min_samples_split=5)\n","    \n","    # Print the Decision Tree\n","    print(\"Decision Tree:\")\n","    print_tree(decision_tree)\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
